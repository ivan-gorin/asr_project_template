{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ovefit asr",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng3UNqKIaAGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad000a2e-39d3-4719-ec2a-7a9099383434"
      },
      "source": [
        "!git clone https://github.com/ivan-gorin/asr_project_template.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'asr_project_template'...\n",
            "remote: Enumerating objects: 237, done.\u001b[K\n",
            "remote: Counting objects: 100% (237/237), done.\u001b[K\n",
            "remote: Compressing objects: 100% (155/155), done.\u001b[K\n",
            "remote: Total 237 (delta 114), reused 197 (delta 76), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (237/237), 775.06 KiB | 7.38 MiB/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iwjF1sZdJyB",
        "outputId": "e08a63f6-262b-43f3-a26f-6777db627c93"
      },
      "source": [
        "!cd asr_project_template && git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating 2fb4f0c..cbcbd50\n",
            "Fast-forward\n",
            " hw_asr/configs/overfit_config.json | 5 \u001b[32m+++\u001b[m\u001b[31m--\u001b[m\n",
            " hw_asr/trainer/trainer.py          | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 2 files changed, 4 insertions(+), 3 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWY_QdLZaBHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d9baeb-7b1c-4f49-9dfb-807d19845434"
      },
      "source": [
        "!pip install -r ./asr_project_template/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 1)) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 2)) (0.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 4)) (4.62.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 5)) (2.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 6)) (3.2.2)\n",
            "Collecting ctcdecode\n",
            "  Downloading ctcdecode-1.0.2.tar.gz (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.4-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 24.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 10)) (1.1.5)\n",
            "Collecting speechbrain~=0.5.9\n",
            "  Downloading speechbrain-0.5.10-py3-none-any.whl (393 kB)\n",
            "\u001b[K     |████████████████████████████████| 393 kB 45.2 MB/s \n",
            "\u001b[?25hCollecting torchaudio~=0.9.1\n",
            "  Downloading torchaudio-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 35.6 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.13.2-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 42.6 MB/s \n",
            "\u001b[?25hCollecting torch_audiomentations\n",
            "  Downloading torch_audiomentations-0.9.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 15)) (0.5.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from speechbrain~=0.5.9->-r ./asr_project_template/requirements.txt (line 11)) (1.0.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from speechbrain~=0.5.9->-r ./asr_project_template/requirements.txt (line 11)) (21.0)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting hyperpyyaml\n",
            "  Downloading HyperPyYAML-1.0.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from speechbrain~=0.5.9->-r ./asr_project_template/requirements.txt (line 11)) (1.4.1)\n",
            "Collecting torch\n",
            "  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 6.9 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r ./asr_project_template/requirements.txt (line 1)) (3.7.4.3)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 33.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r ./asr_project_template/requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.41.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (0.37.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (3.3.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (3.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r ./asr_project_template/requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r ./asr_project_template/requirements.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r ./asr_project_template/requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r ./asr_project_template/requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r ./asr_project_template/requirements.txt (line 8)) (7.1.2)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->-r ./asr_project_template/requirements.txt (line 8)) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r ./asr_project_template/requirements.txt (line 8)) (2.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r ./asr_project_template/requirements.txt (line 8)) (5.4.8)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.3-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 45.4 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb->-r ./asr_project_template/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r ./asr_project_template/requirements.txt (line 10)) (2018.9)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->-r ./asr_project_template/requirements.txt (line 13)) (0.70.12.2)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 40.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->-r ./asr_project_template/requirements.txt (line 13)) (0.3.4)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 42.1 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->-r ./asr_project_template/requirements.txt (line 13)) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain~=0.5.9->-r ./asr_project_template/requirements.txt (line 11)) (3.3.0)\n",
            "Collecting torch-pitch-shift>=1.2.0\n",
            "  Downloading torch_pitch_shift-1.2.0-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (0.8.1)\n",
            "Collecting julius<0.3,>=0.2.3\n",
            "  Downloading julius-0.2.5.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (1.5.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (2.1.9)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (0.51.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (0.22.2.post1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (0.10.3.post1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (0.34.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (1.4.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (2.20)\n",
            "Collecting primePy>=1.3\n",
            "  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r ./asr_project_template/requirements.txt (line 13)) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 41.8 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 46.0 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml>=0.15\n",
            "  Downloading ruamel.yaml-0.17.16-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 51.8 MB/s \n",
            "\u001b[?25hCollecting PyYAML\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 38.5 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.1.2\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 43.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (3.6.0)\n",
            "Building wheels for collected packages: ctcdecode, subprocess32, julius, pathtools\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for ctcdecode\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for ctcdecode\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=234520763c20d183312e7d9177d9df6762fba9941815538eaeeffc2b88cc0377\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.5-py3-none-any.whl size=20813 sha256=c3ebfc27ba10eaf0c60247746b41fc4668794c2feca27a062c0f11c39c5de586\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/37/21/5237732ab2e24f4f033aab47c4b988ce106c22a55c5df8b5cf\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=85cc8f59aaafa4ce1224539764e003e84a5b147c2618b7a49e1810d2c9f9d796\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 julius pathtools\n",
            "Failed to build ctcdecode\n",
            "Installing collected packages: multidict, yarl, torch, smmap, ruamel.yaml.clib, async-timeout, torchaudio, ruamel.yaml, PyYAML, primePy, gitdb, fsspec, aiohttp, yaspin, xxhash, torch-pitch-shift, subprocess32, shortuuid, sentry-sdk, sentencepiece, pathtools, julius, hyperpyyaml, huggingface-hub, GitPython, docker-pycreds, configparser, wandb, torchvision, torch-audiomentations, speechbrain, datasets, ctcdecode\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu111\n",
            "    Uninstalling torch-1.9.0+cu111:\n",
            "      Successfully uninstalled torch-1.9.0+cu111\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu111\n",
            "    Uninstalling torchvision-0.10.0+cu111:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu111\n",
            "    Running setup.py install for ctcdecode ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-6xifjrmq/ctcdecode_c8ba73dea161464abb755e9ba08820d0/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-6xifjrmq/ctcdecode_c8ba73dea161464abb755e9ba08820d0/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-b5cl_oqz/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/ctcdecode Check the logs for full command output.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcs2NKQDEKF2",
        "outputId": "a749590a-a750-47ff-9c7f-64a2d03cad4c"
      },
      "source": [
        "!git clone --recursive https://github.com/parlance/ctcdecode.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 1102 (delta 16), reused 28 (delta 13), pack-reused 1063\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 780.91 KiB | 5.70 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 14047, done.        \n",
            "remote: Counting objects: 100% (360/360), done.        \n",
            "remote: Compressing objects: 100% (292/292), done.        \n",
            "remote: Total 14047 (delta 107), reused 121 (delta 55), pack-reused 13687        \n",
            "Receiving objects: 100% (14047/14047), 5.76 MiB | 14.14 MiB/s, done.\n",
            "Resolving deltas: 100% (7987/7987), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFW9-ARIEQf9",
        "outputId": "2e548df3-d3e5-4867-83ce-65e21c3b4e5d"
      },
      "source": [
        "!cd ctcdecode && pip install .\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/ctcdecode\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: ctcdecode\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctcdecode: filename=ctcdecode-1.0.3-cp37-cp37m-linux_x86_64.whl size=13220392 sha256=1522e28a454772074c6afe71bacf7c60c5bff37a2ee63b93d413fb0e01747ec2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ujo8zczd/wheels/da/bb/b4/233de9fd7927245208e27bcf688bf5680ae3f3874be2895eef\n",
            "Successfully built ctcdecode\n",
            "Installing collected packages: ctcdecode\n",
            "Successfully installed ctcdecode-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0BwlhEody6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5b6bce78-39df-451c-b6e9-010aa1bc34c1"
      },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9QWHahHFtDE",
        "outputId": "564d3b16-9e16-40af-feae-8304010da936"
      },
      "source": [
        "!python asr_project_template/train.py -c asr_project_template/hw_asr/configs/overfit_config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
            "13243 (46.4%) records are longer then 200 characters. Excluding them.\n",
            "Filtered 13243(46.4%) records  from dataset\n",
            "BaselineGRU(\n",
            "  (gru): GRU(128, 512, num_layers=3, batch_first=True, dropout=0.1)\n",
            "  (net): Sequential(\n",
            "    (0): ReLU()\n",
            "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=512, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Trainable parameters: 4415004\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivan-gorin\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-snowflake-51\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ivan-gorin/asr_project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ivan-gorin/asr_project/runs/3jqhokqa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20211015_101943-3jqhokqa\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 1 [0/30 (0%)] Loss: 15.813219\n",
            "train:  33% 10/30 [00:21<00:42,  2.14s/it]Train Epoch: 1 [10/30 (33%)] Loss: 10.556174\n",
            "train:  67% 20/30 [00:44<00:21,  2.18s/it]Train Epoch: 1 [20/30 (67%)] Loss: 3.707997\n",
            "train:  97% 29/30 [01:06<00:02,  2.29s/it]\n",
            "    epoch          : 1\n",
            "    loss           : 8.147733306884765\n",
            "    grad norm      : 8.43595773379008\n",
            "    WER (argmax)   : 1.0508812272581276\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    CER (argmax)   : 0.9861072535543843\n",
            "Train Epoch: 2 [0/30 (0%)] Loss: 4.111337\n",
            "train:  33% 10/30 [00:22<00:43,  2.15s/it]Train Epoch: 2 [10/30 (33%)] Loss: 3.022270\n",
            "train:  67% 20/30 [00:44<00:21,  2.16s/it]Train Epoch: 2 [20/30 (67%)] Loss: 3.123321\n",
            "train:  97% 29/30 [01:04<00:02,  2.15s/it]    epoch          : 2\n",
            "train:  97% 29/30 [01:06<00:02,  2.30s/it]    loss           : 3.203632950782776\n",
            "\n",
            "    grad norm      : 7.459395305315653\n",
            "    WER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    CER (argmax)   : 1.0\n",
            "Train Epoch: 3 [0/30 (0%)] Loss: 2.925421\n",
            "train:  33% 10/30 [00:22<00:43,  2.16s/it]Train Epoch: 3 [10/30 (33%)] Loss: 2.959799\n",
            "train:  67% 20/30 [00:44<00:21,  2.16s/it]Train Epoch: 3 [20/30 (67%)] Loss: 2.912800\n",
            "train:  97% 29/30 [01:06<00:02,  2.30s/it]\n",
            "    epoch          : 3\n",
            "    loss           : 2.9305144707361857\n",
            "    grad norm      : 3.6783892353375753\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 4 [0/30 (0%)] Loss: 2.874273\n",
            "train:  33% 10/30 [00:22<00:43,  2.17s/it]Train Epoch: 4 [10/30 (33%)] Loss: 2.891483\n",
            "train:  67% 20/30 [00:44<00:21,  2.19s/it]Train Epoch: 4 [20/30 (67%)] Loss: 2.877248\n",
            "train:  97% 29/30 [01:05<00:02,  2.18s/it]    epoch          : 4\n",
            "    loss           : 2.879724971453349\n",
            "train:  97% 29/30 [01:07<00:02,  2.32s/it]\n",
            "    grad norm      : 1.4678480515877406\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 5 [0/30 (0%)] Loss: 2.871042\n",
            "train:  33% 10/30 [00:22<00:43,  2.16s/it]Train Epoch: 5 [10/30 (33%)] Loss: 2.869747\n",
            "train:  67% 20/30 [00:44<00:21,  2.17s/it]Train Epoch: 5 [20/30 (67%)] Loss: 2.867690\n",
            "train:  97% 29/30 [01:07<00:02,  2.31s/it]\n",
            "    epoch          : 5\n",
            "    loss           : 2.8699805974960326\n",
            "    grad norm      : 0.7631933783491452\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Saving checkpoint: saved/models/overfit_config/1015_10_19_40:707/checkpoint-epoch5.pth ...\n",
            "Train Epoch: 6 [0/30 (0%)] Loss: 2.866690\n",
            "train:  33% 10/30 [00:22<00:44,  2.24s/it]Train Epoch: 6 [10/30 (33%)] Loss: 2.865607\n",
            "train:  67% 20/30 [00:44<00:21,  2.16s/it]Train Epoch: 6 [20/30 (67%)] Loss: 2.864180\n",
            "train:  97% 29/30 [01:04<00:02,  2.15s/it]    epoch          : 6\n",
            "    loss           : 2.8649845918019614\n",
            "train:  97% 29/30 [01:06<00:02,  2.31s/it]\n",
            "    grad norm      : 0.3338110332687696\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 7 [0/30 (0%)] Loss: 2.863064\n",
            "train:  33% 10/30 [00:22<00:44,  2.25s/it]Train Epoch: 7 [10/30 (33%)] Loss: 2.861968\n",
            "train:  67% 20/30 [00:44<00:21,  2.16s/it]Train Epoch: 7 [20/30 (67%)] Loss: 2.860810\n",
            "train:  97% 29/30 [01:04<00:02,  2.16s/it]    epoch          : 7\n",
            "train:  97% 29/30 [01:06<00:02,  2.30s/it]\n",
            "    loss           : 2.8614898840586345\n",
            "    grad norm      : 0.1439600981771946\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 8 [0/30 (0%)] Loss: 2.859848\n",
            "train:  33% 10/30 [00:22<00:45,  2.25s/it]Train Epoch: 8 [10/30 (33%)] Loss: 2.858734\n",
            "train:  67% 20/30 [00:44<00:21,  2.15s/it]Train Epoch: 8 [20/30 (67%)] Loss: 2.857413\n",
            "train:  97% 29/30 [01:04<00:02,  2.12s/it]    epoch          : 8\n",
            "train:  97% 29/30 [01:06<00:02,  2.30s/it]\n",
            "    loss           : 2.858106541633606\n",
            "    grad norm      : 0.056258782061437765\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 9 [0/30 (0%)] Loss: 2.856068\n",
            "train:  33% 10/30 [00:22<00:42,  2.15s/it]Train Epoch: 9 [10/30 (33%)] Loss: 2.854315\n",
            "train:  67% 20/30 [00:44<00:21,  2.16s/it]Train Epoch: 9 [20/30 (67%)] Loss: 2.852259\n",
            "train:  97% 29/30 [01:06<00:02,  2.30s/it]\n",
            "    epoch          : 9\n",
            "    loss           : 2.853336723645528\n",
            "    grad norm      : 0.04064573689053456\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "Train Epoch: 10 [0/30 (0%)] Loss: 2.849984\n",
            "train:  33% 10/30 [00:22<00:43,  2.15s/it]Train Epoch: 10 [10/30 (33%)] Loss: 2.847337\n",
            "train:  67% 20/30 [00:44<00:21,  2.17s/it]Train Epoch: 10 [20/30 (67%)] Loss: 2.844427\n",
            "train:  97% 29/30 [01:06<00:02,  2.30s/it]\n",
            "    epoch          : 10\n",
            "    loss           : 2.8459362268447874\n",
            "    grad norm      : 0.11458132825791836\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "Saving checkpoint: saved/models/overfit_config/1015_10_19_40:707/checkpoint-epoch10.pth ...\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 11 [0/30 (0%)] Loss: 2.841111\n",
            "train:  33% 10/30 [00:22<00:43,  2.15s/it]Train Epoch: 11 [10/30 (33%)] Loss: 2.837520\n",
            "train:  67% 20/30 [00:44<00:21,  2.17s/it]Train Epoch: 11 [20/30 (67%)] Loss: 2.833060\n",
            "train:  97% 29/30 [01:06<00:02,  2.30s/it]\n",
            "    epoch          : 11\n",
            "    loss           : 2.8349801937739056\n",
            "    grad norm      : 0.32033750768750907\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 12 [0/30 (0%)] Loss: 2.827362\n",
            "train:  33% 10/30 [00:22<00:43,  2.17s/it]Train Epoch: 12 [10/30 (33%)] Loss: 2.822216\n",
            "train:  67% 20/30 [00:44<00:21,  2.19s/it]Train Epoch: 12 [20/30 (67%)] Loss: 2.813459\n",
            "train:  97% 29/30 [01:04<00:02,  2.18s/it]    epoch          : 12\n",
            "train:  97% 29/30 [01:06<00:02,  2.31s/it]\n",
            "    loss           : 2.81811687151591\n",
            "    grad norm      : 0.5549362840751807\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 13 [0/30 (0%)] Loss: 2.804504\n",
            "train:  33% 10/30 [00:22<00:43,  2.16s/it]Train Epoch: 13 [10/30 (33%)] Loss: 2.794949\n",
            "train:  67% 20/30 [00:44<00:21,  2.16s/it]Train Epoch: 13 [20/30 (67%)] Loss: 2.811535\n",
            "train:  97% 29/30 [01:06<00:02,  2.30s/it]\n",
            "    epoch          : 13\n",
            "    loss           : 2.7922001838684083\n",
            "    grad norm      : 0.9723587267100811\n",
            "    WER (argmax)   : 0.9992708333333338\n",
            "    CER (argmax)   : 0.9958943755152415\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 14 [0/30 (0%)] Loss: 2.773622\n",
            "train:  33% 10/30 [00:22<00:43,  2.16s/it]Train Epoch: 14 [10/30 (33%)] Loss: 2.744939\n",
            "train:  67% 20/30 [00:44<00:21,  2.19s/it]Train Epoch: 14 [20/30 (67%)] Loss: 2.796889\n",
            "train:  97% 29/30 [01:07<00:02,  2.33s/it]    epoch          : 14\n",
            "\n",
            "    loss           : 2.750853085517883\n",
            "    grad norm      : 0.8350155969460805\n",
            "    WER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    CER (argmax)   : 0.9920277811795588\n",
            "Train Epoch: 15 [0/30 (0%)] Loss: 2.723839\n",
            "train:  33% 10/30 [00:22<00:43,  2.17s/it]Train Epoch: 15 [10/30 (33%)] Loss: 2.689510\n",
            "train:  67% 20/30 [00:44<00:21,  2.19s/it]Train Epoch: 15 [20/30 (67%)] Loss: 2.716265\n",
            "train:  97% 29/30 [01:07<00:02,  2.33s/it]\n",
            "    epoch          : 15\n",
            "    loss           : 2.6925097624460856\n",
            "    grad norm      : 0.9351055016120274\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 0.9877841706880603\n",
            "Saving checkpoint: saved/models/overfit_config/1015_10_19_40:707/checkpoint-epoch15.pth ...\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 16 [0/30 (0%)] Loss: 2.650129\n",
            "train:  33% 10/30 [00:22<00:43,  2.18s/it]Train Epoch: 16 [10/30 (33%)] Loss: 2.601541\n",
            "train:  67% 20/30 [00:44<00:21,  2.18s/it]Train Epoch: 16 [20/30 (67%)] Loss: 2.587406\n",
            "train:  97% 29/30 [01:05<00:02,  2.18s/it]    epoch          : 16\n",
            "train:  97% 29/30 [01:07<00:02,  2.32s/it]\n",
            "    loss           : 2.6110160827636717\n",
            "    grad norm      : 1.3636879533529283\n",
            "    WER (argmax)   : 0.9999099099099098\n",
            "    CER (argmax)   : 0.9816335342396424\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 17 [0/30 (0%)] Loss: 2.553151\n",
            "train:  33% 10/30 [00:22<00:43,  2.18s/it]Train Epoch: 17 [10/30 (33%)] Loss: 2.458918\n",
            "train:  67% 20/30 [00:45<00:21,  2.18s/it]Train Epoch: 17 [20/30 (67%)] Loss: 2.478627\n",
            "train:  97% 29/30 [01:07<00:02,  2.33s/it]    epoch          : 17\n",
            "    loss           : 2.457308793067932\n",
            "    grad norm      : 1.6762308100859324\n",
            "    WER (argmax)   : 0.9975697424179398\n",
            "    CER (argmax)   : 0.974213592233813\n",
            "\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 18 [0/30 (0%)] Loss: 2.332447\n",
            "train:  33% 10/30 [00:22<00:45,  2.27s/it]Train Epoch: 18 [10/30 (33%)] Loss: 2.181466\n",
            "train:  67% 20/30 [00:45<00:21,  2.20s/it]Train Epoch: 18 [20/30 (67%)] Loss: 2.377517\n",
            "train:  97% 29/30 [01:05<00:02,  2.20s/it]    epoch          : 18\n",
            "train:  97% 29/30 [01:07<00:02,  2.34s/it]\n",
            "    loss           : 2.2932431221008303\n",
            "    grad norm      : 2.6023864726225536\n",
            "    WER (argmax)   : 0.9937460239921206\n",
            "    CER (argmax)   : 0.960794441282361\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 19 [0/30 (0%)] Loss: 2.077778\n",
            "train:  33% 10/30 [00:22<00:43,  2.18s/it]Train Epoch: 19 [10/30 (33%)] Loss: 2.047308\n",
            "train:  67% 20/30 [00:45<00:22,  2.21s/it]Train Epoch: 19 [20/30 (67%)] Loss: 1.705921\n",
            "train:  97% 29/30 [01:07<00:02,  2.34s/it]    epoch          : 19\n",
            "\n",
            "    loss           : 1.8176289280255635\n",
            "    grad norm      : 2.6457039217154183\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    WER (argmax)   : 0.9924890721295395\n",
            "    CER (argmax)   : 0.9034719992148043\n",
            "Train Epoch: 20 [0/30 (0%)] Loss: 1.618506\n",
            "train:  33% 10/30 [00:22<00:43,  2.19s/it]Train Epoch: 20 [10/30 (33%)] Loss: 1.394506\n",
            "train:  67% 20/30 [00:45<00:21,  2.19s/it]Train Epoch: 20 [20/30 (67%)] Loss: 1.066706\n",
            "train:  97% 29/30 [01:08<00:02,  2.35s/it]\n",
            "    epoch          : 20\n",
            "    loss           : 1.2874890089035034\n",
            "    grad norm      : 2.8435083786646524\n",
            "    WER (argmax)   : 0.9849153920021189\n",
            "    CER (argmax)   : 0.6917275587382012\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Saving checkpoint: saved/models/overfit_config/1015_10_19_40:707/checkpoint-epoch20.pth ...\n",
            "Train Epoch: 21 [0/30 (0%)] Loss: 0.959538\n",
            "train:  33% 10/30 [00:22<00:43,  2.19s/it]Train Epoch: 21 [10/30 (33%)] Loss: 0.742702\n",
            "train:  67% 20/30 [00:45<00:21,  2.20s/it]Train Epoch: 21 [20/30 (67%)] Loss: 0.562370\n",
            "train:  97% 29/30 [01:05<00:02,  2.20s/it]    epoch          : 21\n",
            "train:  97% 29/30 [01:08<00:02,  2.35s/it]\n",
            "    loss           : 0.6866186658541361\n",
            "    grad norm      : 2.4196638107299804\n",
            "    WER (argmax)   : 0.846069762813351\n",
            "    CER (argmax)   : 0.328207137323602\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 22 [0/30 (0%)] Loss: 0.421788\n",
            "train:  33% 10/30 [00:22<00:43,  2.19s/it]Train Epoch: 22 [10/30 (33%)] Loss: 0.304987\n",
            "train:  67% 20/30 [00:45<00:21,  2.19s/it]Train Epoch: 22 [20/30 (67%)] Loss: 0.250219\n",
            "train:  97% 29/30 [01:05<00:02,  2.20s/it]    epoch          : 22\n",
            "train:  97% 29/30 [01:07<00:02,  2.33s/it]\n",
            "    loss           : 0.29603022734324136\n",
            "    grad norm      : 0.9592339237531026\n",
            "    WER (argmax)   : 0.382461245544168\n",
            "    CER (argmax)   : 0.11445772291754717\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 23 [0/30 (0%)] Loss: 0.256110\n",
            "train:  33% 10/30 [00:22<00:44,  2.23s/it]Train Epoch: 23 [10/30 (33%)] Loss: 5.696183\n",
            "train:  67% 20/30 [00:45<00:21,  2.20s/it]Train Epoch: 23 [20/30 (67%)] Loss: 2.380429\n",
            "train:  97% 29/30 [01:08<00:02,  2.35s/it]\n",
            "    epoch          : 23\n",
            "    loss           : 2.0114124059677123\n",
            "    grad norm      : 4.852490625778834\n",
            "    WER (argmax)   : 0.8220261202021735\n",
            "    CER (argmax)   : 0.4905563066150741\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 24 [0/30 (0%)] Loss: 1.103363\n",
            "train:  33% 10/30 [00:22<00:44,  2.23s/it]Train Epoch: 24 [10/30 (33%)] Loss: 0.540055\n",
            "train:  67% 20/30 [00:46<00:22,  2.23s/it]Train Epoch: 24 [20/30 (67%)] Loss: 0.316612\n",
            "train:  97% 29/30 [01:06<00:02,  2.20s/it]    epoch          : 24\n",
            "    loss           : 0.5050494755307834\n",
            "train:  97% 29/30 [01:08<00:02,  2.36s/it]\n",
            "    grad norm      : 0.9147809048493704\n",
            "    WER (argmax)   : 0.5636491711263971\n",
            "    CER (argmax)   : 0.21412896546671706\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 25 [0/30 (0%)] Loss: 0.222583\n",
            "train:  33% 10/30 [00:22<00:43,  2.19s/it]Train Epoch: 25 [10/30 (33%)] Loss: 0.180315\n",
            "train:  67% 20/30 [00:45<00:21,  2.19s/it]Train Epoch: 25 [20/30 (67%)] Loss: 0.153420\n",
            "train:  97% 29/30 [01:05<00:02,  2.22s/it]    epoch          : 25\n",
            "train:  97% 29/30 [01:07<00:02,  2.34s/it]    loss           : 0.1742465650041898\n",
            "\n",
            "    grad norm      : 0.3866682082414627\n",
            "    WER (argmax)   : 0.18203125160057743\n",
            "    CER (argmax)   : 0.06206933692683687\n",
            "Saving checkpoint: saved/models/overfit_config/1015_10_19_40:707/checkpoint-epoch25.pth ...\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 26 [0/30 (0%)] Loss: 0.145027\n",
            "train:  33% 10/30 [00:22<00:43,  2.20s/it]Train Epoch: 26 [10/30 (33%)] Loss: 0.139078\n",
            "train:  67% 20/30 [00:45<00:22,  2.21s/it]Train Epoch: 26 [20/30 (67%)] Loss: 0.130679\n",
            "train:  97% 29/30 [01:07<00:02,  2.34s/it]    epoch          : 26\n",
            "\n",
            "    loss           : 0.1374941314260165\n",
            "    grad norm      : 0.4676921139160792\n",
            "    WER (argmax)   : 0.13588521480547716\n",
            "    CER (argmax)   : 0.04935916212636778\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 27 [0/30 (0%)] Loss: 0.166346\n",
            "train:  33% 10/30 [00:22<00:44,  2.21s/it]Train Epoch: 27 [10/30 (33%)] Loss: 0.161813\n",
            "train:  67% 20/30 [00:45<00:21,  2.18s/it]Train Epoch: 27 [20/30 (67%)] Loss: 0.408717\n",
            "train:  97% 29/30 [01:07<00:02,  2.34s/it]\n",
            "    epoch          : 27\n",
            "    loss           : 0.27631442695856095\n",
            "    grad norm      : 1.5146532555421193\n",
            "    WER (argmax)   : 0.2969411634503903\n",
            "    CER (argmax)   : 0.0909143300849391\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 28 [0/30 (0%)] Loss: 0.284755\n",
            "train:  33% 10/30 [00:22<00:43,  2.18s/it]Train Epoch: 28 [10/30 (33%)] Loss: 0.230342\n",
            "train:  67% 20/30 [00:45<00:21,  2.20s/it]Train Epoch: 28 [20/30 (67%)] Loss: 0.176321\n",
            "train:  97% 29/30 [01:07<00:02,  2.34s/it]    epoch          : 28\n",
            "    loss           : 0.204094235599041\n",
            "\n",
            "    grad norm      : 0.8945861121018728\n",
            "    WER (argmax)   : 0.23760198617328945\n",
            "    CER (argmax)   : 0.0718132679337467\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 29 [0/30 (0%)] Loss: 0.145876\n",
            "train:  33% 10/30 [00:22<00:43,  2.20s/it]Train Epoch: 29 [10/30 (33%)] Loss: 0.124431\n",
            "train:  67% 20/30 [00:45<00:21,  2.19s/it]Train Epoch: 29 [20/30 (67%)] Loss: 0.117726\n",
            "train:  97% 29/30 [01:08<00:02,  2.35s/it]\n",
            "    epoch          : 29\n",
            "    loss           : 0.12399767388900121\n",
            "    grad norm      : 0.526725756128629\n",
            "    WER (argmax)   : 0.13824984919814465\n",
            "    CER (argmax)   : 0.04685023169953613\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 30 [0/30 (0%)] Loss: 0.112248\n",
            "train:  33% 10/30 [00:22<00:44,  2.22s/it]Train Epoch: 30 [10/30 (33%)] Loss: 0.106010\n",
            "train:  67% 20/30 [00:45<00:21,  2.20s/it]Train Epoch: 30 [20/30 (67%)] Loss: 0.117867\n",
            "train:  97% 29/30 [01:07<00:02,  2.34s/it]    epoch          : 30\n",
            "\n",
            "    loss           : 0.11275782013932864\n",
            "    grad norm      : 0.5412029107411702\n",
            "    WER (argmax)   : 0.12419197756282113\n",
            "    CER (argmax)   : 0.043410737500740475\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Saving checkpoint: saved/models/overfit_config/1015_10_19_40:707/checkpoint-epoch30.pth ...\n",
            "Train Epoch: 31 [0/30 (0%)] Loss: 0.102629\n",
            "train:  33% 10/30 [00:23<00:45,  2.27s/it]Train Epoch: 31 [10/30 (33%)] Loss: 0.124688\n",
            "train:  67% 20/30 [00:45<00:22,  2.21s/it]Train Epoch: 31 [20/30 (67%)] Loss: 0.108371\n",
            "train:  97% 29/30 [01:08<00:02,  2.36s/it]    epoch          : 31\n",
            "    loss           : 0.1158575544754664\n",
            "\n",
            "    grad norm      : 0.6158285677433014\n",
            "    WER (argmax)   : 0.1134776205848012\n",
            "    CER (argmax)   : 0.040692757297655376\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 32 [0/30 (0%)] Loss: 0.157835\n",
            "train:  33% 10/30 [00:22<00:44,  2.21s/it]Train Epoch: 32 [10/30 (33%)] Loss: 0.174716\n",
            "train:  67% 20/30 [00:45<00:22,  2.20s/it]Train Epoch: 32 [20/30 (67%)] Loss: 0.148904\n",
            "train:  97% 29/30 [01:08<00:02,  2.36s/it]\n",
            "    epoch          : 32\n",
            "    loss           : 0.14170906965931257\n",
            "    grad norm      : 0.9338146189848582\n",
            "    WER (argmax)   : 0.16848512081321282\n",
            "    CER (argmax)   : 0.05114254605989408\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 33 [0/30 (0%)] Loss: 0.130739\n",
            "train:  33% 10/30 [00:22<00:44,  2.21s/it]Train Epoch: 33 [10/30 (33%)] Loss: 0.147511\n",
            "train:  67% 20/30 [00:45<00:22,  2.20s/it]Train Epoch: 33 [20/30 (67%)] Loss: 0.120043\n",
            "train:  97% 29/30 [01:08<00:02,  2.35s/it]    epoch          : 33\n",
            "\n",
            "    loss           : 0.1291789561510086\n",
            "    grad norm      : 0.9240814030170441\n",
            "    WER (argmax)   : 0.162907033345037\n",
            "    CER (argmax)   : 0.048072201853700716\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 34 [0/30 (0%)] Loss: 0.163267\n",
            "train:  33% 10/30 [00:23<00:44,  2.24s/it]Train Epoch: 34 [10/30 (33%)] Loss: 0.119438\n",
            "train:  67% 20/30 [00:46<00:22,  2.23s/it]Train Epoch: 34 [20/30 (67%)] Loss: 0.101672\n",
            "train:  97% 29/30 [01:06<00:02,  2.23s/it]    epoch          : 34\n",
            "train:  97% 29/30 [01:09<00:02,  2.38s/it]\n",
            "    loss           : 0.1182672768831253\n",
            "    grad norm      : 0.7497498750686645\n",
            "    WER (argmax)   : 0.13892952607424797\n",
            "    CER (argmax)   : 0.04363070316644551\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 35 [0/30 (0%)] Loss: 0.106522\n",
            "train:  33% 10/30 [00:22<00:44,  2.24s/it]Train Epoch: 35 [10/30 (33%)] Loss: 0.108096\n",
            "train:  67% 20/30 [00:46<00:23,  2.31s/it]Train Epoch: 35 [20/30 (67%)] Loss: 0.084612\n",
            "train:  97% 29/30 [01:09<00:02,  2.39s/it]\n",
            "    epoch          : 35\n",
            "    loss           : 0.09776098082462946\n",
            "    grad norm      : 0.6161827971537908\n",
            "    WER (argmax)   : 0.11266543184041407\n",
            "    CER (argmax)   : 0.036498814824802925\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Saving checkpoint: saved/models/overfit_config/1015_10_19_40:707/checkpoint-epoch35.pth ...\n",
            "Train Epoch: 36 [0/30 (0%)] Loss: 0.086072\n",
            "train:  33% 10/30 [00:22<00:44,  2.23s/it]Train Epoch: 36 [10/30 (33%)] Loss: 0.081782\n",
            "train:  67% 20/30 [00:45<00:22,  2.23s/it]Train Epoch: 36 [20/30 (67%)] Loss: 0.077442\n",
            "train:  97% 29/30 [01:08<00:02,  2.38s/it]    epoch          : 36\n",
            "\n",
            "    loss           : 0.0819950816531976\n",
            "    grad norm      : 0.558355313539505\n",
            "    WER (argmax)   : 0.09390915871079812\n",
            "    CER (argmax)   : 0.03140868833571087\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 37 [0/30 (0%)] Loss: 0.074665\n",
            "train:  33% 10/30 [00:23<00:44,  2.24s/it]Train Epoch: 37 [10/30 (33%)] Loss: 0.065620\n",
            "train:  67% 20/30 [00:46<00:22,  2.23s/it]Train Epoch: 37 [20/30 (67%)] Loss: 0.070715\n",
            "train:  97% 29/30 [01:06<00:02,  2.22s/it]    epoch          : 37\n",
            "    loss           : 0.07098092151184877\n",
            "train:  97% 29/30 [01:09<00:02,  2.38s/it]\n",
            "    grad norm      : 0.4929521789153417\n",
            "    WER (argmax)   : 0.08036958909242739\n",
            "    CER (argmax)   : 0.027519339800304257\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 38 [0/30 (0%)] Loss: 0.089356\n",
            "train:  33% 10/30 [00:22<00:44,  2.22s/it]Train Epoch: 38 [10/30 (33%)] Loss: 0.086353\n",
            "train:  67% 20/30 [00:46<00:22,  2.30s/it]Train Epoch: 38 [20/30 (67%)] Loss: 0.101166\n",
            "train:  97% 29/30 [01:06<00:02,  2.22s/it]    epoch          : 38\n",
            "train:  97% 29/30 [01:08<00:02,  2.37s/it]\n",
            "    loss           : 0.09699757397174835\n",
            "    grad norm      : 0.7691717286904652\n",
            "    WER (argmax)   : 0.11006411042676598\n",
            "    CER (argmax)   : 0.03455157408018929\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 39 [0/30 (0%)] Loss: 0.093243\n",
            "train:  33% 10/30 [00:22<00:44,  2.22s/it]Train Epoch: 39 [10/30 (33%)] Loss: 0.150780\n",
            "train:  67% 20/30 [00:45<00:22,  2.22s/it]Train Epoch: 39 [20/30 (67%)] Loss: 0.122697\n",
            "train:  97% 29/30 [01:08<00:02,  2.36s/it]    epoch          : 39\n",
            "\n",
            "    loss           : 0.13707114060719808\n",
            "    grad norm      : 1.0975194871425629\n",
            "    WER (argmax)   : 0.1639195487453794\n",
            "    CER (argmax)   : 0.04609952001852779\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 40 [0/30 (0%)] Loss: 0.147187\n",
            "train:  33% 10/30 [00:23<00:44,  2.24s/it]Train Epoch: 40 [10/30 (33%)] Loss: 0.114542\n",
            "train:  67% 20/30 [00:45<00:22,  2.22s/it]Train Epoch: 40 [20/30 (67%)] Loss: 0.159865\n",
            "train:  97% 29/30 [01:08<00:02,  2.38s/it]\n",
            "    epoch          : 40\n",
            "    loss           : 0.13066502983371417\n",
            "    grad norm      : 1.0233644684155783\n",
            "    WER (argmax)   : 0.16452684099925582\n",
            "    CER (argmax)   : 0.04656249629636386\n",
            "Saving checkpoint: saved/models/overfit_config/1015_10_19_40:707/checkpoint-epoch40.pth ...\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 41 [0/30 (0%)] Loss: 0.127200\n",
            "train:  33% 10/30 [00:22<00:44,  2.23s/it]Train Epoch: 41 [10/30 (33%)] Loss: 0.091820\n",
            "train:  67% 20/30 [00:45<00:22,  2.23s/it]Train Epoch: 41 [20/30 (67%)] Loss: 0.075542\n",
            "train:  97% 29/30 [01:09<00:02,  2.38s/it]\n",
            "    epoch          : 41\n",
            "train:   0% 0/30 [00:00<?, ?it/s]\n",
            "    grad norm      : 0.8391598562399546\n",
            "    WER (argmax)   : 0.12884364338986556\n",
            "    CER (argmax)   : 0.037770334318475936\n",
            "Train Epoch: 42 [0/30 (0%)] Loss: 0.085604\n",
            "train:  33% 10/30 [00:22<00:44,  2.23s/it]Train Epoch: 42 [10/30 (33%)] Loss: 0.085603\n",
            "train:  67% 20/30 [00:45<00:22,  2.23s/it]Train Epoch: 42 [20/30 (67%)] Loss: 0.080785\n",
            "train:  97% 29/30 [01:08<00:02,  2.37s/it]    epoch          : 42\n",
            "\n",
            "    loss           : 0.10048810367782911\n",
            "    grad norm      : 0.9071203649044037\n",
            "    WER (argmax)   : 0.12264572242579655\n",
            "    CER (argmax)   : 0.03513702314956893\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 43 [0/30 (0%)] Loss: 0.101552\n",
            "train:  33% 10/30 [00:23<00:45,  2.28s/it]Train Epoch: 43 [10/30 (33%)] Loss: 0.079529\n",
            "train:  67% 20/30 [00:46<00:22,  2.23s/it]Train Epoch: 43 [20/30 (67%)] Loss: 0.066880\n",
            "train:  97% 29/30 [01:09<00:02,  2.38s/it]    epoch          : 43\n",
            "\n",
            "    loss           : 0.07886772801478704\n",
            "    grad norm      : 0.6571167230606079\n",
            "    WER (argmax)   : 0.10253586535223222\n",
            "    CER (argmax)   : 0.030245635255028446\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 44 [0/30 (0%)] Loss: 0.075723\n",
            "train:  33% 10/30 [00:22<00:44,  2.23s/it]Train Epoch: 44 [10/30 (33%)] Loss: 0.099100\n",
            "train:  67% 20/30 [00:45<00:22,  2.23s/it]Train Epoch: 44 [20/30 (67%)] Loss: 0.091265\n",
            "train:  97% 29/30 [01:09<00:02,  2.38s/it]\n",
            "    epoch          : 44\n",
            "    loss           : 0.12334352446099123\n",
            "    grad norm      : 1.256593213478724\n",
            "    WER (argmax)   : 0.1303854575094424\n",
            "    CER (argmax)   : 0.04338936892943049\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 45 [0/30 (0%)] Loss: 1.199057\n",
            "train:  33% 10/30 [00:23<00:45,  2.25s/it]Train Epoch: 45 [10/30 (33%)] Loss: 1.325406\n",
            "train:  67% 20/30 [00:46<00:22,  2.25s/it]Train Epoch: 45 [20/30 (67%)] Loss: 0.396875\n",
            "train:  97% 29/30 [01:09<00:02,  2.40s/it]    epoch          : 45\n",
            "\n",
            "    loss           : 0.8573950866858164\n",
            "    grad norm      : 3.7609574596087136\n",
            "    WER (argmax)   : 0.5885488669919522\n",
            "    CER (argmax)   : 0.21490610029009552\n",
            "Saving checkpoint: saved/models/overfit_config/1015_10_19_40:707/checkpoint-epoch45.pth ...\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 46 [0/30 (0%)] Loss: 0.237678\n",
            "train:  33% 10/30 [00:23<00:45,  2.25s/it]Train Epoch: 46 [10/30 (33%)] Loss: 0.205202\n",
            "train:  67% 20/30 [00:46<00:22,  2.28s/it]Train Epoch: 46 [20/30 (67%)] Loss: 0.155658\n",
            "train:  97% 29/30 [01:07<00:02,  2.26s/it]    epoch          : 46\n",
            "train:  97% 29/30 [01:10<00:02,  2.41s/it]\n",
            "    loss           : 0.1767721024652322\n",
            "    grad norm      : 1.0155621290206909\n",
            "    WER (argmax)   : 0.22692436735070165\n",
            "    CER (argmax)   : 0.06751650213689565\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 47 [0/30 (0%)] Loss: 0.120931\n",
            "train:  33% 10/30 [00:23<00:45,  2.26s/it]Train Epoch: 47 [10/30 (33%)] Loss: 0.109289\n",
            "train:  67% 20/30 [00:46<00:22,  2.26s/it]Train Epoch: 47 [20/30 (67%)] Loss: 0.093075\n",
            "train:  97% 29/30 [01:09<00:02,  2.40s/it]\n",
            "    epoch          : 47\n",
            "    loss           : 0.1064868782957395\n",
            "    grad norm      : 0.6971142252286275\n",
            "    WER (argmax)   : 0.13810665469695416\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    CER (argmax)   : 0.043358723662186224\n",
            "Train Epoch: 48 [0/30 (0%)] Loss: 0.090905\n",
            "train:  33% 10/30 [00:23<00:45,  2.25s/it]Train Epoch: 48 [10/30 (33%)] Loss: 0.096935\n",
            "train:  67% 20/30 [00:46<00:22,  2.25s/it]Train Epoch: 48 [20/30 (67%)] Loss: 0.097072\n",
            "train:  97% 29/30 [01:07<00:02,  2.25s/it]    epoch          : 48\n",
            "train:  97% 29/30 [01:09<00:02,  2.39s/it]\n",
            "    loss           : 0.09135556221008301\n",
            "    grad norm      : 0.7968642155329386\n",
            "    WER (argmax)   : 0.12191968075255268\n",
            "    CER (argmax)   : 0.03571377511162302\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 49 [0/30 (0%)] Loss: 0.094675\n",
            "train:  33% 10/30 [00:23<00:44,  2.24s/it]Train Epoch: 49 [10/30 (33%)] Loss: 0.092790\n",
            "train:  67% 20/30 [00:46<00:22,  2.24s/it]Train Epoch: 49 [20/30 (67%)] Loss: 0.076216\n",
            "train:  97% 29/30 [01:09<00:02,  2.40s/it]\n",
            "    epoch          : 49\n",
            "    loss           : 0.08594263369838397\n",
            "    grad norm      : 0.775714761018753\n",
            "    WER (argmax)   : 0.12082732967716306\n",
            "    CER (argmax)   : 0.035296843680511315\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 50 [0/30 (0%)] Loss: 0.083963\n",
            "train:  33% 10/30 [00:23<00:45,  2.25s/it]Train Epoch: 50 [10/30 (33%)] Loss: 0.072435\n",
            "train:  67% 20/30 [00:46<00:22,  2.25s/it]Train Epoch: 50 [20/30 (67%)] Loss: 0.077284\n",
            "train:  97% 29/30 [01:09<00:02,  2.40s/it]\n",
            "    epoch          : 50\n",
            "    loss           : 0.07599545121192933\n",
            "    grad norm      : 0.835479998588562\n",
            "    WER (argmax)   : 0.11384703693469192\n",
            "    CER (argmax)   : 0.03251125399792804\n",
            "Saving checkpoint: saved/models/overfit_config/1015_10_19_40:707/checkpoint-epoch50.pth ...\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 38395... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    CER (argmax)_train ███████████████▇▄▂▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    WER (argmax)_train █▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▃▇▂▁▂▂▁▁▂▂▁▁▁▁▂▁▁▁▆▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                epoch_ ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch_train ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       grad norm_train ▆█▅▂▁▁▁▁▁▁▁▁▁▂▃▄▃▂▂▁▁▁▂▁▁▂▂▁▁▂▂▂▂▂▁▆▂▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   learning rate_train ▁▁▁▁▁▂▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇▇██████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_train █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   steps_per_sec_train ▆▇▆▆▅▇█▇▆▇▇▃▅▅▅▅▂▅▄▅▄▅▄▄▄▄▄▃▃▃▄▂▂▃▃▁▁▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    CER (argmax)_train 0.0328\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    WER (argmax)_train 0.11503\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                epoch_ 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch_train 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       grad norm_train 0.84083\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   learning rate_train 0.00385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_train 0.07762\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   steps_per_sec_train 0.04314\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 450 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mefficient-snowflake-51\u001b[0m: \u001b[34mhttps://wandb.ai/ivan-gorin/asr_project/runs/3jqhokqa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211015_101943-3jqhokqa/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
          ]
        }
      ]
    }
  ]
}