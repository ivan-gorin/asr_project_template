{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ovefit asr",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng3UNqKIaAGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238b07be-ef11-44d2-fdff-14d724c62f56"
      },
      "source": [
        "!git clone https://github.com/ivan-gorin/asr_project_template.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'asr_project_template'...\n",
            "remote: Enumerating objects: 298, done.\u001b[K\n",
            "remote: Counting objects: 100% (298/298), done.\u001b[K\n",
            "remote: Compressing objects: 100% (188/188), done.\u001b[K\n",
            "remote: Total 298 (delta 159), reused 239 (delta 104), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (298/298), 794.40 KiB | 7.22 MiB/s, done.\n",
            "Resolving deltas: 100% (159/159), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iwjF1sZdJyB",
        "outputId": "22466a6e-f775-4e12-dfc9-703767bf9c41"
      },
      "source": [
        "!cd asr_project_template && git pull"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/13)\u001b[K\rremote: Counting objects:  15% (2/13)\u001b[K\rremote: Counting objects:  23% (3/13)\u001b[K\rremote: Counting objects:  30% (4/13)\u001b[K\rremote: Counting objects:  38% (5/13)\u001b[K\rremote: Counting objects:  46% (6/13)\u001b[K\rremote: Counting objects:  53% (7/13)\u001b[K\rremote: Counting objects:  61% (8/13)\u001b[K\rremote: Counting objects:  69% (9/13)\u001b[K\rremote: Counting objects:  76% (10/13)\u001b[K\rremote: Counting objects:  84% (11/13)\u001b[K\rremote: Counting objects:  92% (12/13)\u001b[K\rremote: Counting objects: 100% (13/13)\u001b[K\rremote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects:  33% (1/3)\u001b[K\rremote: Compressing objects:  66% (2/3)\u001b[K\rremote: Compressing objects: 100% (3/3)\u001b[K\rremote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 7 (delta 4), reused 7 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects:  14% (1/7)   \rUnpacking objects:  28% (2/7)   \rUnpacking objects:  42% (3/7)   \rUnpacking objects:  57% (4/7)   \rUnpacking objects:  71% (5/7)   \rUnpacking objects:  85% (6/7)   \rUnpacking objects: 100% (7/7)   \rUnpacking objects: 100% (7/7), done.\n",
            "From https://github.com/ivan-gorin/asr_project_template\n",
            "   bb4b3ac..756875d  hw_asr_2021 -> origin/hw_asr_2021\n",
            "Updating bb4b3ac..756875d\n",
            "Fast-forward\n",
            " hw_asr/configs/overfit_config.json | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " hw_asr/model/baseline_model.py     | 6 \u001b[32m+++++\u001b[m\u001b[31m-\u001b[m\n",
            " 2 files changed, 6 insertions(+), 2 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWY_QdLZaBHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acdb8d23-60f3-406f-a14a-8a89afb16e62"
      },
      "source": [
        "!pip install -r ./asr_project_template/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 1)) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 2)) (0.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 4)) (4.62.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 5)) (2.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 6)) (3.2.2)\n",
            "Collecting ctcdecode\n",
            "  Downloading ctcdecode-1.0.2.tar.gz (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.4-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 29.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 10)) (1.1.5)\n",
            "Collecting speechbrain~=0.5.9\n",
            "  Downloading speechbrain-0.5.10-py3-none-any.whl (393 kB)\n",
            "\u001b[K     |████████████████████████████████| 393 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting torchaudio~=0.9.1\n",
            "  Downloading torchaudio-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 36.7 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 35.4 MB/s \n",
            "\u001b[?25hCollecting torch_audiomentations\n",
            "  Downloading torch_audiomentations-0.9.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from -r ./asr_project_template/requirements.txt (line 15)) (0.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from speechbrain~=0.5.9->-r ./asr_project_template/requirements.txt (line 11)) (1.4.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 37.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from speechbrain~=0.5.9->-r ./asr_project_template/requirements.txt (line 11)) (21.0)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting hyperpyyaml\n",
            "  Downloading HyperPyYAML-1.0.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from speechbrain~=0.5.9->-r ./asr_project_template/requirements.txt (line 11)) (1.0.1)\n",
            "Collecting torch\n",
            "  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 7.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r ./asr_project_template/requirements.txt (line 1)) (3.7.4.3)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 63 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r ./asr_project_template/requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.41.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (0.37.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (3.3.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r ./asr_project_template/requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r ./asr_project_template/requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r ./asr_project_template/requirements.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r ./asr_project_template/requirements.txt (line 6)) (1.3.2)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.3-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 42.4 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 29.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r ./asr_project_template/requirements.txt (line 8)) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r ./asr_project_template/requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->-r ./asr_project_template/requirements.txt (line 8)) (3.13)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r ./asr_project_template/requirements.txt (line 8)) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb->-r ./asr_project_template/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r ./asr_project_template/requirements.txt (line 10)) (2018.9)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 46.5 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->-r ./asr_project_template/requirements.txt (line 13)) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->-r ./asr_project_template/requirements.txt (line 13)) (0.70.12.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->-r ./asr_project_template/requirements.txt (line 13)) (0.3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain~=0.5.9->-r ./asr_project_template/requirements.txt (line 11)) (3.3.0)\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (0.8.1)\n",
            "Collecting torch-pitch-shift>=1.2.0\n",
            "  Downloading torch_pitch_shift-1.2.0-py3-none-any.whl (4.0 kB)\n",
            "Collecting julius<0.3,>=0.2.3\n",
            "  Downloading julius-0.2.5.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (0.2.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (1.5.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (0.22.2.post1)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (0.10.3.post1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (0.51.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (0.34.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (1.4.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations->-r ./asr_project_template/requirements.txt (line 14)) (2.20)\n",
            "Collecting primePy>=1.3\n",
            "  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 41.9 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r ./asr_project_template/requirements.txt (line 13)) (21.2.0)\n",
            "Collecting ruamel.yaml>=0.15\n",
            "  Downloading ruamel.yaml-0.17.16-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 47.1 MB/s \n",
            "\u001b[?25hCollecting PyYAML\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 39.6 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.1.2\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->-r ./asr_project_template/requirements.txt (line 5)) (3.6.0)\n",
            "Building wheels for collected packages: ctcdecode, subprocess32, julius, pathtools\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for ctcdecode\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for ctcdecode\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=cb9d322c206a809f3d3b352a82984bb553b7ba71f8f8bd6674d8f1d0464f8dfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.5-py3-none-any.whl size=20813 sha256=a5e3d0df9a732371497cd37713313bed9e0bc9b942c97cacb155cb03a82c9d99\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/37/21/5237732ab2e24f4f033aab47c4b988ce106c22a55c5df8b5cf\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=169e28e2b93d89aebe7793e5b0f241bff4ff11e3565bfc9fd96ffb3b8ac5bc44\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 julius pathtools\n",
            "Failed to build ctcdecode\n",
            "Installing collected packages: multidict, yarl, torch, smmap, ruamel.yaml.clib, async-timeout, torchaudio, ruamel.yaml, PyYAML, primePy, gitdb, fsspec, aiohttp, yaspin, xxhash, torch-pitch-shift, subprocess32, shortuuid, sentry-sdk, sentencepiece, pathtools, julius, hyperpyyaml, huggingface-hub, GitPython, docker-pycreds, configparser, wandb, torchvision, torch-audiomentations, speechbrain, datasets, ctcdecode\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu111\n",
            "    Uninstalling torch-1.9.0+cu111:\n",
            "      Successfully uninstalled torch-1.9.0+cu111\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu111\n",
            "    Uninstalling torchvision-0.10.0+cu111:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu111\n",
            "    Running setup.py install for ctcdecode ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-wkcj8kdp/ctcdecode_cb0ff428561a4302ae13886938cf1c08/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-wkcj8kdp/ctcdecode_cb0ff428561a4302ae13886938cf1c08/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-cgeci_xc/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/ctcdecode Check the logs for full command output.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcs2NKQDEKF2",
        "outputId": "f4f9cdb8-b2af-41cf-f551-8579ca13516f"
      },
      "source": [
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!cd ctcdecode && pip install ."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/39)\u001b[K\rremote: Counting objects:   5% (2/39)\u001b[K\rremote: Counting objects:   7% (3/39)\u001b[K\rremote: Counting objects:  10% (4/39)\u001b[K\rremote: Counting objects:  12% (5/39)\u001b[K\rremote: Counting objects:  15% (6/39)\u001b[K\rremote: Counting objects:  17% (7/39)\u001b[K\rremote: Counting objects:  20% (8/39)\u001b[K\rremote: Counting objects:  23% (9/39)\u001b[K\rremote: Counting objects:  25% (10/39)\u001b[K\rremote: Counting objects:  28% (11/39)\u001b[K\rremote: Counting objects:  30% (12/39)\u001b[K\rremote: Counting objects:  33% (13/39)\u001b[K\rremote: Counting objects:  35% (14/39)\u001b[K\rremote: Counting objects:  38% (15/39)\u001b[K\rremote: Counting objects:  41% (16/39)\u001b[K\rremote: Counting objects:  43% (17/39)\u001b[K\rremote: Counting objects:  46% (18/39)\u001b[K\rremote: Counting objects:  48% (19/39)\u001b[K\rremote: Counting objects:  51% (20/39)\u001b[K\rremote: Counting objects:  53% (21/39)\u001b[K\rremote: Counting objects:  56% (22/39)\u001b[K\rremote: Counting objects:  58% (23/39)\u001b[K\rremote: Counting objects:  61% (24/39)\u001b[K\rremote: Counting objects:  64% (25/39)\u001b[K\rremote: Counting objects:  66% (26/39)\u001b[K\rremote: Counting objects:  69% (27/39)\u001b[K\rremote: Counting objects:  71% (28/39)\u001b[K\rremote: Counting objects:  74% (29/39)\u001b[K\rremote: Counting objects:  76% (30/39)\u001b[K\rremote: Counting objects:  79% (31/39)\u001b[K\rremote: Counting objects:  82% (32/39)\u001b[K\rremote: Counting objects:  84% (33/39)\u001b[K\rremote: Counting objects:  87% (34/39)\u001b[K\rremote: Counting objects:  89% (35/39)\u001b[K\rremote: Counting objects:  92% (36/39)\u001b[K\rremote: Counting objects:  94% (37/39)\u001b[K\rremote: Counting objects:  97% (38/39)\u001b[K\rremote: Counting objects: 100% (39/39)\u001b[K\rremote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 1102 (delta 16), reused 28 (delta 13), pack-reused 1063\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 780.91 KiB | 6.85 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 14047, done.        \n",
            "remote: Counting objects: 100% (360/360), done.        \n",
            "remote: Compressing objects: 100% (292/292), done.        \n",
            "remote: Total 14047 (delta 107), reused 121 (delta 55), pack-reused 13687        \n",
            "Receiving objects: 100% (14047/14047), 5.76 MiB | 15.72 MiB/s, done.\n",
            "Resolving deltas: 100% (7987/7987), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "Processing /content/ctcdecode\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: ctcdecode\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctcdecode: filename=ctcdecode-1.0.3-cp37-cp37m-linux_x86_64.whl size=13192804 sha256=d2f4a476c8e12554b4ce189ac08b92b1da7b41a3851cdb28953ddb26617cc6d0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ekfsmx4l/wheels/da/bb/b4/233de9fd7927245208e27bcf688bf5680ae3f3874be2895eef\n",
            "Successfully built ctcdecode\n",
            "Installing collected packages: ctcdecode\n",
            "Successfully installed ctcdecode-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0BwlhEody6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "923b7e94-fda8-47c9-b199-0db035e762bc"
      },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9QWHahHFtDE",
        "outputId": "6554cf13-8cf8-41e4-fc98-1fb0ca4da0c9"
      },
      "source": [
        "!python asr_project_template/train.py -c asr_project_template/hw_asr/configs/overfit_config.json"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
            "13243 (46.4%) records are longer then 200 characters. Excluding them.\n",
            "Filtered 13243(46.4%) records  from dataset\n",
            "BaselineGRU(\n",
            "  (gru): GRU(128, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (net): Sequential(\n",
            "    (0): ReLU()\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=256, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Trainable parameters: 2700572\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivan-gorin\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcool-sea-60\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ivan-gorin/asr_project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ivan-gorin/asr_project/runs/24xhrfs7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20211016_164920-24xhrfs7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 1 [0/30 (0%)] Loss: 16.829840\n",
            "train:  33% 10/30 [00:15<00:30,  1.51s/it]Train Epoch: 1 [10/30 (33%)] Loss: 13.587773\n",
            "train:  67% 20/30 [00:31<00:15,  1.57s/it]Train Epoch: 1 [20/30 (67%)] Loss: 4.856106\n",
            "train:  97% 29/30 [00:45<00:01,  1.53s/it]    epoch          : 1\n",
            "train:  97% 29/30 [00:47<00:01,  1.64s/it]    loss           : 9.824911419550578\n",
            "\n",
            "    grad norm      : 8.443698358535766\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 0.993876913340199\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 2 [0/30 (0%)] Loss: 4.457445\n",
            "train:  33% 10/30 [00:16<00:30,  1.53s/it]Train Epoch: 2 [10/30 (33%)] Loss: 3.990689\n",
            "train:  67% 20/30 [00:32<00:15,  1.57s/it]Train Epoch: 2 [20/30 (67%)] Loss: 3.031934\n",
            "train:  97% 29/30 [00:48<00:01,  1.67s/it]\n",
            "    epoch          : 2\n",
            "    loss           : 3.66482191880544\n",
            "    grad norm      : 8.204847892125448\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 3 [0/30 (0%)] Loss: 3.235348\n",
            "train:  33% 10/30 [00:16<00:31,  1.56s/it]Train Epoch: 3 [10/30 (33%)] Loss: 2.943146\n",
            "train:  67% 20/30 [00:32<00:15,  1.56s/it]Train Epoch: 3 [20/30 (67%)] Loss: 3.003925\n",
            "train:  97% 29/30 [00:46<00:01,  1.55s/it]    epoch          : 3\n",
            "train:  97% 29/30 [00:48<00:01,  1.67s/it]    loss           : 3.0139233907063803\n",
            "\n",
            "    grad norm      : 5.013452643156052\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 4 [0/30 (0%)] Loss: 2.917752\n",
            "train:  33% 10/30 [00:16<00:31,  1.56s/it]Train Epoch: 4 [10/30 (33%)] Loss: 2.934555\n",
            "train:  67% 20/30 [00:32<00:15,  1.57s/it]Train Epoch: 4 [20/30 (67%)] Loss: 2.895520\n",
            "train:  97% 29/30 [00:47<00:01,  1.55s/it]    epoch          : 4\n",
            "train:  97% 29/30 [00:48<00:01,  1.68s/it]\n",
            "    loss           : 2.9122960408528646\n",
            "    grad norm      : 2.1278312812248865\n",
            "    WER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    CER (argmax)   : 1.0\n",
            "Train Epoch: 5 [0/30 (0%)] Loss: 2.903571\n",
            "train:  33% 10/30 [00:16<00:31,  1.57s/it]Train Epoch: 5 [10/30 (33%)] Loss: 2.892501\n",
            "train:  67% 20/30 [00:32<00:15,  1.56s/it]Train Epoch: 5 [20/30 (67%)] Loss: 2.885913\n",
            "train:  97% 29/30 [00:47<00:01,  1.56s/it]    epoch          : 5\n",
            "train:  97% 29/30 [00:48<00:01,  1.68s/it]\n",
            "    loss           : 2.8908201456069946\n",
            "    grad norm      : 1.0875834534565607\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Saving checkpoint: saved/models/overfit_config/1016_16_49_17:654/checkpoint-epoch5.pth ...\n",
            "Train Epoch: 6 [0/30 (0%)] Loss: 2.885795\n",
            "train:  33% 10/30 [00:16<00:30,  1.55s/it]Train Epoch: 6 [10/30 (33%)] Loss: 2.883745\n",
            "train:  67% 20/30 [00:32<00:15,  1.55s/it]Train Epoch: 6 [20/30 (67%)] Loss: 2.881121\n",
            "train:  97% 29/30 [00:46<00:01,  1.57s/it]    epoch          : 6\n",
            "train:  97% 29/30 [00:48<00:01,  1.67s/it]\n",
            "    loss           : 2.881965676943461\n",
            "    grad norm      : 0.4764480464160442\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 7 [0/30 (0%)] Loss: 2.879272\n",
            "train:  33% 10/30 [00:16<00:31,  1.55s/it]Train Epoch: 7 [10/30 (33%)] Loss: 2.877496\n",
            "train:  67% 20/30 [00:32<00:16,  1.62s/it]Train Epoch: 7 [20/30 (67%)] Loss: 2.875773\n",
            "train:  97% 29/30 [00:47<00:01,  1.57s/it]    epoch          : 7\n",
            "train:  97% 29/30 [00:49<00:01,  1.69s/it]\n",
            "    loss           : 2.8766100804011026\n",
            "    grad norm      : 0.21486675478518008\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "Train Epoch: 8 [0/30 (0%)] Loss: 2.873636\n",
            "train:  33% 10/30 [00:16<00:31,  1.56s/it]Train Epoch: 8 [10/30 (33%)] Loss: 2.871442\n",
            "train:  67% 20/30 [00:32<00:15,  1.56s/it]Train Epoch: 8 [20/30 (67%)] Loss: 2.868821\n",
            "train:  97% 29/30 [00:47<00:01,  1.55s/it]    epoch          : 8\n",
            "train:  97% 29/30 [00:48<00:01,  1.68s/it]\n",
            "    loss           : 2.8701573530832927\n",
            "    grad norm      : 0.0812316165616115\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 9 [0/30 (0%)] Loss: 2.865763\n",
            "train:  33% 10/30 [00:16<00:31,  1.56s/it]Train Epoch: 9 [10/30 (33%)] Loss: 2.862364\n",
            "train:  67% 20/30 [00:32<00:15,  1.58s/it]Train Epoch: 9 [20/30 (67%)] Loss: 2.858598\n",
            "train:  97% 29/30 [00:46<00:01,  1.56s/it]    epoch          : 9\n",
            "train:  97% 29/30 [00:48<00:01,  1.67s/it]\n",
            "    loss           : 2.860571304957072\n",
            "    grad norm      : 0.0434421398366491\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 10 [0/30 (0%)] Loss: 2.854398\n",
            "train:  33% 10/30 [00:16<00:31,  1.55s/it]Train Epoch: 10 [10/30 (33%)] Loss: 2.849630\n",
            "train:  67% 20/30 [00:32<00:16,  1.64s/it]Train Epoch: 10 [20/30 (67%)] Loss: 2.843752\n",
            "train:  97% 29/30 [00:47<00:01,  1.56s/it]    epoch          : 10\n",
            "train:  97% 29/30 [00:48<00:01,  1.68s/it]    loss           : 2.846733546257019\n",
            "\n",
            "    grad norm      : 0.040067407054205736\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "Saving checkpoint: saved/models/overfit_config/1016_16_49_17:654/checkpoint-epoch10.pth ...\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 11 [0/30 (0%)] Loss: 2.836743\n",
            "train:  33% 10/30 [00:16<00:30,  1.55s/it]Train Epoch: 11 [10/30 (33%)] Loss: 2.828488\n",
            "train:  67% 20/30 [00:32<00:15,  1.56s/it]Train Epoch: 11 [20/30 (67%)] Loss: 2.818449\n",
            "train:  97% 29/30 [00:48<00:01,  1.68s/it]\n",
            "    epoch          : 11\n",
            "    loss           : 2.8238306045532227\n",
            "    grad norm      : 0.2262513836224874\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "Train Epoch: 12 [0/30 (0%)] Loss: 2.806987\n",
            "train:  33% 10/30 [00:16<00:31,  1.57s/it]Train Epoch: 12 [10/30 (33%)] Loss: 2.794171\n",
            "train:  67% 20/30 [00:32<00:15,  1.56s/it]Train Epoch: 12 [20/30 (67%)] Loss: 2.780047\n",
            "train:  97% 29/30 [00:48<00:01,  1.69s/it]\n",
            "    epoch          : 12\n",
            "    loss           : 2.7872828245162964\n",
            "    grad norm      : 0.4108633190393448\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 13 [0/30 (0%)] Loss: 2.765326\n",
            "train:  33% 10/30 [00:16<00:31,  1.55s/it]Train Epoch: 13 [10/30 (33%)] Loss: 2.744620\n",
            "train:  67% 20/30 [00:32<00:15,  1.58s/it]Train Epoch: 13 [20/30 (67%)] Loss: 2.724164\n",
            "train:  97% 29/30 [00:47<00:01,  1.56s/it]    epoch          : 13\n",
            "train:  97% 29/30 [00:48<00:01,  1.68s/it]\n",
            "    loss           : 2.733555626869202\n",
            "    grad norm      : 0.5739007463057836\n",
            "    WER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    CER (argmax)   : 1.0\n",
            "Train Epoch: 14 [0/30 (0%)] Loss: 2.697700\n",
            "train:  33% 10/30 [00:16<00:31,  1.56s/it]Train Epoch: 14 [10/30 (33%)] Loss: 2.684945\n",
            "train:  67% 20/30 [00:32<00:15,  1.57s/it]Train Epoch: 14 [20/30 (67%)] Loss: 2.649191\n",
            "train:  97% 29/30 [00:48<00:01,  1.68s/it]    epoch          : 14\n",
            "\n",
            "    loss           : 2.6646524111429852\n",
            "    grad norm      : 0.8871159811814626\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 1.0\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 15 [0/30 (0%)] Loss: 2.602849\n",
            "train:  33% 10/30 [00:16<00:31,  1.57s/it]Train Epoch: 15 [10/30 (33%)] Loss: 2.552813\n",
            "train:  67% 20/30 [00:32<00:15,  1.59s/it]Train Epoch: 15 [20/30 (67%)] Loss: 2.509435\n",
            "train:  97% 29/30 [00:49<00:01,  1.70s/it]\n",
            "    epoch          : 15\n",
            "    loss           : 2.5409945090611776\n",
            "    grad norm      : 0.9658014327287674\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 0.9998098448368113\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Saving checkpoint: saved/models/overfit_config/1016_16_49_17:654/checkpoint-epoch15.pth ...\n",
            "Train Epoch: 16 [0/30 (0%)] Loss: 2.444041\n",
            "train:  33% 10/30 [00:16<00:33,  1.65s/it]Train Epoch: 16 [10/30 (33%)] Loss: 2.385059\n",
            "train:  67% 20/30 [00:32<00:15,  1.58s/it]Train Epoch: 16 [20/30 (67%)] Loss: 2.286507\n",
            "train:  97% 29/30 [00:49<00:01,  1.70s/it]\n",
            "    epoch          : 16\n",
            "    loss           : 2.348925479253133\n",
            "    grad norm      : 1.6021210173765819\n",
            "    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 0.9968918714517457\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 17 [0/30 (0%)] Loss: 2.168434\n",
            "train:  33% 10/30 [00:16<00:31,  1.58s/it]Train Epoch: 17 [10/30 (33%)] Loss: 2.161700\n",
            "train:  67% 20/30 [00:32<00:15,  1.60s/it]Train Epoch: 17 [20/30 (67%)] Loss: 1.976200\n",
            "train:  97% 29/30 [00:49<00:01,  1.70s/it]    epoch          : 17\n",
            "\n",
            "    loss           : 2.0602856159210203\n",
            "    grad norm      : 1.9974466502666473\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    WER (argmax)   : 1.0\n",
            "    CER (argmax)   : 0.9764062659411635\n",
            "Train Epoch: 18 [0/30 (0%)] Loss: 1.805395\n",
            "train:  33% 10/30 [00:16<00:31,  1.57s/it]Train Epoch: 18 [10/30 (33%)] Loss: 1.637970\n",
            "train:  67% 20/30 [00:32<00:16,  1.60s/it]Train Epoch: 18 [20/30 (67%)] Loss: 1.616727\n",
            "train:  97% 29/30 [00:47<00:01,  1.59s/it]    epoch          : 18\n",
            "train:  97% 29/30 [00:49<00:01,  1.71s/it]\n",
            "    loss           : 1.6694594542185466\n",
            "    grad norm      : 3.471978799502055\n",
            "    WER (argmax)   : 0.9995421638744008\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    CER (argmax)   : 0.8766033651442057\n",
            "Train Epoch: 19 [0/30 (0%)] Loss: 1.476955\n",
            "train:  33% 10/30 [00:16<00:31,  1.59s/it]Train Epoch: 19 [10/30 (33%)] Loss: 1.174779\n",
            "train:  67% 20/30 [00:33<00:16,  1.61s/it]Train Epoch: 19 [20/30 (67%)] Loss: 0.930836\n",
            "train:  97% 29/30 [00:48<00:01,  1.66s/it]    epoch          : 19\n",
            "train:  97% 29/30 [00:50<00:01,  1.72s/it]\n",
            "    loss           : 1.0842809538046518\n",
            "    grad norm      : 1.7287928978602092\n",
            "    WER (argmax)   : 0.9690899649467899\n",
            "    CER (argmax)   : 0.6280204781477715\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 20 [0/30 (0%)] Loss: 0.741067\n",
            "train:  33% 10/30 [00:16<00:31,  1.60s/it]Train Epoch: 20 [10/30 (33%)] Loss: 0.593918\n",
            "train:  67% 20/30 [00:33<00:16,  1.61s/it]Train Epoch: 20 [20/30 (67%)] Loss: 0.449386\n",
            "train:  97% 29/30 [00:49<00:01,  1.71s/it]\n",
            "    epoch          : 20\n",
            "    loss           : 0.5478859364986419\n",
            "    grad norm      : 1.5253692130247751\n",
            "    WER (argmax)   : 0.7148210271339701\n",
            "    CER (argmax)   : 0.26730518655795804\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Saving checkpoint: saved/models/overfit_config/1016_16_49_17:654/checkpoint-epoch20.pth ...\n",
            "Train Epoch: 21 [0/30 (0%)] Loss: 0.423950\n",
            "train:  33% 10/30 [00:17<00:32,  1.65s/it]Train Epoch: 21 [10/30 (33%)] Loss: 0.305352\n",
            "train:  67% 20/30 [00:33<00:16,  1.62s/it]Train Epoch: 21 [20/30 (67%)] Loss: 0.238570\n",
            "train:  97% 29/30 [00:50<00:01,  1.74s/it]\n",
            "    epoch          : 21\n",
            "    loss           : 0.28430608411629993\n",
            "    grad norm      : 0.8834294219811757\n",
            "    WER (argmax)   : 0.35826842016550625\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    CER (argmax)   : 0.11066988128165137\n",
            "Train Epoch: 22 [0/30 (0%)] Loss: 0.199645\n",
            "train:  33% 10/30 [00:16<00:32,  1.61s/it]Train Epoch: 22 [10/30 (33%)] Loss: 0.177694\n",
            "train:  67% 20/30 [00:33<00:15,  1.59s/it]Train Epoch: 22 [20/30 (67%)] Loss: 0.161066\n",
            "train:  97% 29/30 [00:48<00:01,  1.61s/it]    epoch          : 22\n",
            "train:  97% 29/30 [00:50<00:01,  1.73s/it]\n",
            "    loss           : 0.17099110782146454\n",
            "    grad norm      : 0.5669269055128098\n",
            "    WER (argmax)   : 0.1982579722151149\n",
            "    CER (argmax)   : 0.06492933582624746\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 23 [0/30 (0%)] Loss: 0.147437\n",
            "train:  33% 10/30 [00:16<00:32,  1.61s/it]Train Epoch: 23 [10/30 (33%)] Loss: 4.109397\n",
            "train:  67% 20/30 [00:33<00:15,  1.59s/it]Train Epoch: 23 [20/30 (67%)] Loss: 2.640389\n",
            "train:  97% 29/30 [00:50<00:01,  1.73s/it]\n",
            "    epoch          : 23\n",
            "    loss           : 2.137678701678912\n",
            "    grad norm      : 5.728529028097788\n",
            "    WER (argmax)   : 0.7789833763638494\n",
            "    CER (argmax)   : 0.4531423581542935\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 24 [0/30 (0%)] Loss: 1.055594\n",
            "train:  33% 10/30 [00:16<00:32,  1.63s/it]Train Epoch: 24 [10/30 (33%)] Loss: 0.579539\n",
            "train:  67% 20/30 [00:33<00:16,  1.61s/it]Train Epoch: 24 [20/30 (67%)] Loss: 0.359355\n",
            "train:  97% 29/30 [00:50<00:01,  1.75s/it]\n",
            "    epoch          : 24\n",
            "    loss           : 0.5311125069856644\n",
            "    grad norm      : 0.6991235534350078\n",
            "    WER (argmax)   : 0.6125736298896206\n",
            "    CER (argmax)   : 0.22604794949576032\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 25 [0/30 (0%)] Loss: 0.253341\n",
            "train:  33% 10/30 [00:16<00:32,  1.64s/it]Train Epoch: 25 [10/30 (33%)] Loss: 0.193421\n",
            "train:  67% 20/30 [00:33<00:16,  1.64s/it]Train Epoch: 25 [20/30 (67%)] Loss: 0.155095\n",
            "train:  97% 29/30 [00:50<00:01,  1.75s/it]\n",
            "    epoch          : 25\n",
            "    loss           : 0.1806914711991946\n",
            "    grad norm      : 0.38373002807299295\n",
            "    WER (argmax)   : 0.20938823316732333\n",
            "    CER (argmax)   : 0.0653343747730919\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Saving checkpoint: saved/models/overfit_config/1016_16_49_17:654/checkpoint-epoch25.pth ...\n",
            "Train Epoch: 26 [0/30 (0%)] Loss: 0.131008\n",
            "train:  33% 10/30 [00:17<00:33,  1.70s/it]Train Epoch: 26 [10/30 (33%)] Loss: 0.113556\n",
            "train:  67% 20/30 [00:33<00:16,  1.60s/it]Train Epoch: 26 [20/30 (67%)] Loss: 0.226361\n",
            "train:  97% 29/30 [00:50<00:01,  1.74s/it]\n",
            "    epoch          : 26\n",
            "    loss           : 0.15233786776661873\n",
            "    grad norm      : 0.6560862575968106\n",
            "    WER (argmax)   : 0.15123795923824138\n",
            "    CER (argmax)   : 0.04892840294864509\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 27 [0/30 (0%)] Loss: 0.181751\n",
            "train:  33% 10/30 [00:16<00:32,  1.61s/it]Train Epoch: 27 [10/30 (33%)] Loss: 0.166494\n",
            "train:  67% 20/30 [00:33<00:16,  1.61s/it]Train Epoch: 27 [20/30 (67%)] Loss: 0.194877\n",
            "train:  97% 29/30 [00:50<00:01,  1.73s/it]\n",
            "    epoch          : 27\n",
            "    loss           : 0.1631588637828827\n",
            "    grad norm      : 0.8236195802688598\n",
            "    WER (argmax)   : 0.1706117689283988\n",
            "    CER (argmax)   : 0.05103821465996836\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 28 [0/30 (0%)] Loss: 0.122345\n",
            "train:  33% 10/30 [00:16<00:32,  1.63s/it]Train Epoch: 28 [10/30 (33%)] Loss: 0.105640\n",
            "train:  67% 20/30 [00:33<00:15,  1.60s/it]Train Epoch: 28 [20/30 (67%)] Loss: 0.095246\n",
            "train:  97% 29/30 [00:50<00:01,  1.73s/it]    epoch          : 28\n",
            "\n",
            "    loss           : 0.10262068957090378\n",
            "    grad norm      : 0.3971534560124079\n",
            "    WER (argmax)   : 0.0928533590774275\n",
            "    CER (argmax)   : 0.03307379419334284\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 29 [0/30 (0%)] Loss: 0.082002\n",
            "train:  33% 10/30 [00:17<00:32,  1.61s/it]Train Epoch: 29 [10/30 (33%)] Loss: 0.089104\n",
            "train:  67% 20/30 [00:33<00:16,  1.61s/it]Train Epoch: 29 [20/30 (67%)] Loss: 0.095952\n",
            "train:  97% 29/30 [00:48<00:01,  1.64s/it]    epoch          : 29\n",
            "    loss           : 0.09321060081322988\n",
            "train:  97% 29/30 [00:50<00:01,  1.74s/it]\n",
            "    grad norm      : 0.4215581571062406\n",
            "    WER (argmax)   : 0.09054617079317687\n",
            "    CER (argmax)   : 0.03224346228877754\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 30 [0/30 (0%)] Loss: 0.097354\n",
            "train:  33% 10/30 [00:17<00:32,  1.64s/it]Train Epoch: 30 [10/30 (33%)] Loss: 0.103095\n",
            "train:  67% 20/30 [00:34<00:16,  1.63s/it]Train Epoch: 30 [20/30 (67%)] Loss: 0.113580\n",
            "train:  97% 29/30 [00:49<00:01,  1.63s/it]    epoch          : 30\n",
            "train:  97% 29/30 [00:51<00:01,  1.76s/it]\n",
            "    loss           : 0.10806051914890608\n",
            "    grad norm      : 0.625344009200732\n",
            "    WER (argmax)   : 0.11135235475183453\n",
            "    CER (argmax)   : 0.03698986714120931\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Saving checkpoint: saved/models/overfit_config/1016_16_49_17:654/checkpoint-epoch30.pth ...\n",
            "Train Epoch: 31 [0/30 (0%)] Loss: 0.106132\n",
            "train:  33% 10/30 [00:16<00:32,  1.62s/it]Train Epoch: 31 [10/30 (33%)] Loss: 0.100233\n",
            "train:  67% 20/30 [00:33<00:16,  1.62s/it]Train Epoch: 31 [20/30 (67%)] Loss: 0.114772\n",
            "train:  97% 29/30 [00:50<00:01,  1.75s/it]    epoch          : 31\n",
            "\n",
            "    loss           : 0.11278904924790065\n",
            "    grad norm      : 0.8394248604774475\n",
            "    WER (argmax)   : 0.13034494268840105\n",
            "    CER (argmax)   : 0.03977341635707356\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 32 [0/30 (0%)] Loss: 0.116811\n",
            "train:  33% 10/30 [00:16<00:32,  1.63s/it]Train Epoch: 32 [10/30 (33%)] Loss: 0.107592\n",
            "train:  67% 20/30 [00:33<00:16,  1.63s/it]Train Epoch: 32 [20/30 (67%)] Loss: 0.108471\n",
            "train:  97% 29/30 [00:50<00:01,  1.75s/it]\n",
            "    epoch          : 32\n",
            "    loss           : 0.11314185733596484\n",
            "    grad norm      : 0.8410691599051158\n",
            "    WER (argmax)   : 0.13536490395369113\n",
            "    CER (argmax)   : 0.04020689113339759\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 33 [0/30 (0%)] Loss: 0.112179\n",
            "train:  33% 10/30 [00:17<00:34,  1.70s/it]Train Epoch: 33 [10/30 (33%)] Loss: 0.095677\n",
            "train:  67% 20/30 [00:34<00:16,  1.63s/it]Train Epoch: 33 [20/30 (67%)] Loss: 0.112293\n",
            "train:  97% 29/30 [00:50<00:01,  1.76s/it]\n",
            "    epoch          : 33\n",
            "    loss           : 0.10479299301902453\n",
            "    grad norm      : 0.9119295716285706\n",
            "    WER (argmax)   : 0.1298389915938882\n",
            "    CER (argmax)   : 0.037566450515628666\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 34 [0/30 (0%)] Loss: 0.107024\n",
            "train:  33% 10/30 [00:16<00:32,  1.61s/it]Train Epoch: 34 [10/30 (33%)] Loss: 0.114585\n",
            "train:  67% 20/30 [00:33<00:16,  1.60s/it]Train Epoch: 34 [20/30 (67%)] Loss: 0.098377\n",
            "train:  97% 29/30 [00:48<00:01,  1.63s/it]    epoch          : 34\n",
            "train:  97% 29/30 [00:50<00:01,  1.74s/it]\n",
            "    loss           : 0.10242329761385918\n",
            "    grad norm      : 0.8449066599210103\n",
            "    WER (argmax)   : 0.1242927626794236\n",
            "    CER (argmax)   : 0.038205357183148644\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 35 [0/30 (0%)] Loss: 0.087511\n",
            "train:  33% 10/30 [00:16<00:32,  1.62s/it]Train Epoch: 35 [10/30 (33%)] Loss: 0.100569\n",
            "train:  67% 20/30 [00:33<00:16,  1.63s/it]Train Epoch: 35 [20/30 (67%)] Loss: 0.086571\n",
            "train:  97% 29/30 [00:48<00:01,  1.62s/it]    epoch          : 35\n",
            "train:  97% 29/30 [00:50<00:01,  1.75s/it]    loss           : 0.09142355248332024\n",
            "    grad norm      : 0.6722292770942052\n",
            "\n",
            "    WER (argmax)   : 0.10264280635253605\n",
            "    CER (argmax)   : 0.032837693984735045\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Saving checkpoint: saved/models/overfit_config/1016_16_49_17:654/checkpoint-epoch35.pth ...\n",
            "Train Epoch: 36 [0/30 (0%)] Loss: 0.090133\n",
            "train:  33% 10/30 [00:17<00:33,  1.68s/it]Train Epoch: 36 [10/30 (33%)] Loss: 0.108617\n",
            "train:  67% 20/30 [00:34<00:16,  1.63s/it]Train Epoch: 36 [20/30 (67%)] Loss: 0.097029\n",
            "train:  97% 29/30 [00:49<00:01,  1.62s/it]    epoch          : 36\n",
            "    loss           : 0.0951547106107076\n",
            "train:  97% 29/30 [00:50<00:01,  1.76s/it]\n",
            "    grad norm      : 0.8435997446378072\n",
            "    WER (argmax)   : 0.11328420094413798\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    CER (argmax)   : 0.03445692252663316\n",
            "Train Epoch: 37 [0/30 (0%)] Loss: 0.115727\n",
            "train:  33% 10/30 [00:16<00:32,  1.65s/it]Train Epoch: 37 [10/30 (33%)] Loss: 0.106883\n",
            "train:  67% 20/30 [00:33<00:16,  1.63s/it]Train Epoch: 37 [20/30 (67%)] Loss: 0.100077\n",
            "train:  97% 29/30 [00:49<00:01,  1.66s/it]    epoch          : 37\n",
            "train:  97% 29/30 [00:51<00:01,  1.76s/it]\n",
            "    loss           : 0.10591621994972229\n",
            "    grad norm      : 0.9673981606960297\n",
            "    WER (argmax)   : 0.13386002503406935\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    CER (argmax)   : 0.03806056144866366\n",
            "Train Epoch: 38 [0/30 (0%)] Loss: 0.098940\n",
            "train:  33% 10/30 [00:17<00:32,  1.63s/it]Train Epoch: 38 [10/30 (33%)] Loss: 0.097292\n",
            "train:  67% 20/30 [00:33<00:16,  1.63s/it]Train Epoch: 38 [20/30 (67%)] Loss: 0.089139\n",
            "train:  97% 29/30 [00:50<00:01,  1.75s/it]    epoch          : 38\n",
            "\n",
            "    loss           : 0.141274094581604\n",
            "    grad norm      : 1.3324652234713237\n",
            "    WER (argmax)   : 0.16042722628159953\n",
            "    CER (argmax)   : 0.04460379587600432\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 39 [0/30 (0%)] Loss: 0.467378\n",
            "train:  33% 10/30 [00:17<00:35,  1.76s/it]Train Epoch: 39 [10/30 (33%)] Loss: 0.578844\n",
            "train:  67% 20/30 [00:34<00:16,  1.64s/it]Train Epoch: 39 [20/30 (67%)] Loss: 0.225012\n",
            "train:  97% 29/30 [00:50<00:01,  1.76s/it]\n",
            "    epoch          : 39\n",
            "    loss           : 0.47597385197877884\n",
            "    grad norm      : 3.031306246916453\n",
            "    WER (argmax)   : 0.454343412697639\n",
            "    CER (argmax)   : 0.1417034624759194\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 40 [0/30 (0%)] Loss: 0.136574\n",
            "train:  33% 10/30 [00:16<00:32,  1.64s/it]Train Epoch: 40 [10/30 (33%)] Loss: 0.101957\n",
            "train:  67% 20/30 [00:34<00:16,  1.65s/it]Train Epoch: 40 [20/30 (67%)] Loss: 0.086267\n",
            "train:  97% 29/30 [00:51<00:01,  1.77s/it]\n",
            "    epoch          : 40\n",
            "    loss           : 0.09883993268013\n",
            "    grad norm      : 0.4674512316783269\n",
            "    WER (argmax)   : 0.11146149285668774\n",
            "    CER (argmax)   : 0.03806098053923365\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Saving checkpoint: saved/models/overfit_config/1016_16_49_17:654/checkpoint-epoch40.pth ...\n",
            "Train Epoch: 41 [0/30 (0%)] Loss: 0.072223\n",
            "train:  33% 10/30 [00:16<00:32,  1.63s/it]Train Epoch: 41 [10/30 (33%)] Loss: 0.069905\n",
            "train:  67% 20/30 [00:33<00:16,  1.63s/it]Train Epoch: 41 [20/30 (67%)] Loss: 0.067560\n",
            "train:  97% 29/30 [00:50<00:01,  1.75s/it]\n",
            "    epoch          : 41\n",
            "    loss           : 0.06815771932403246\n",
            "    grad norm      : 0.23546618471542993\n",
            "    WER (argmax)   : 0.0685588849481681\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    CER (argmax)   : 0.03545275840019261\n",
            "Train Epoch: 42 [0/30 (0%)] Loss: 0.063723\n",
            "train:  33% 10/30 [00:16<00:32,  1.63s/it]Train Epoch: 42 [10/30 (33%)] Loss: 0.078614\n",
            "train:  67% 20/30 [00:34<00:16,  1.64s/it]Train Epoch: 42 [20/30 (67%)] Loss: 0.068432\n",
            "train:  97% 29/30 [00:49<00:01,  1.65s/it]    epoch          : 42\n",
            "train:  97% 29/30 [00:51<00:01,  1.76s/it]\n",
            "    loss           : 0.07148370519280434\n",
            "    grad norm      : 0.46035802314678825\n",
            "    WER (argmax)   : 0.07643230472934368\n",
            "    CER (argmax)   : 0.0354303528920718\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 43 [0/30 (0%)] Loss: 0.067983\n",
            "train:  33% 10/30 [00:17<00:32,  1.64s/it]Train Epoch: 43 [10/30 (33%)] Loss: 0.064241\n",
            "train:  67% 20/30 [00:34<00:16,  1.65s/it]Train Epoch: 43 [20/30 (67%)] Loss: 0.062484\n",
            "train:  97% 29/30 [00:51<00:01,  1.76s/it]\n",
            "    epoch          : 43\n",
            "    loss           : 0.06391731550296148\n",
            "    grad norm      : 0.29722733596960704\n",
            "    WER (argmax)   : 0.06602966210259573\n",
            "    CER (argmax)   : 0.028729987586788953\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 44 [0/30 (0%)] Loss: 0.056543\n",
            "train:  33% 10/30 [00:17<00:32,  1.65s/it]Train Epoch: 44 [10/30 (33%)] Loss: 0.059531\n",
            "train:  67% 20/30 [00:34<00:16,  1.63s/it]Train Epoch: 44 [20/30 (67%)] Loss: 0.062418\n",
            "train:  97% 29/30 [00:51<00:01,  1.77s/it]\n",
            "    epoch          : 44\n",
            "    loss           : 0.05753692016005516\n",
            "    grad norm      : 0.20522504995266597\n",
            "    WER (argmax)   : 0.0567089327397953\n",
            "    CER (argmax)   : 0.02726659864404613\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 45 [0/30 (0%)] Loss: 0.052845\n",
            "train:  33% 10/30 [00:17<00:33,  1.66s/it]Train Epoch: 45 [10/30 (33%)] Loss: 0.049924\n",
            "train:  67% 20/30 [00:34<00:16,  1.63s/it]Train Epoch: 45 [20/30 (67%)] Loss: 0.051261\n",
            "train:  97% 29/30 [00:51<00:01,  1.77s/it]\n",
            "    epoch          : 45\n",
            "    loss           : 0.0511037769416968\n",
            "    grad norm      : 0.16655569026867548\n",
            "    WER (argmax)   : 0.050746625285155264\n",
            "    CER (argmax)   : 0.024699600557290813\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Saving checkpoint: saved/models/overfit_config/1016_16_49_17:654/checkpoint-epoch45.pth ...\n",
            "Train Epoch: 46 [0/30 (0%)] Loss: 0.046861\n",
            "train:  33% 10/30 [00:16<00:32,  1.65s/it]Train Epoch: 46 [10/30 (33%)] Loss: 0.051236\n",
            "train:  67% 20/30 [00:33<00:16,  1.64s/it]Train Epoch: 46 [20/30 (67%)] Loss: 0.049713\n",
            "train:  97% 29/30 [00:51<00:01,  1.76s/it]\n",
            "    epoch          : 46\n",
            "    loss           : 0.04946683707336585\n",
            "    grad norm      : 0.1840435708562533\n",
            "    WER (argmax)   : 0.04948175794241457\n",
            "    CER (argmax)   : 0.02264809824085578\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 47 [0/30 (0%)] Loss: 0.047161\n",
            "train:  33% 10/30 [00:17<00:32,  1.64s/it]Train Epoch: 47 [10/30 (33%)] Loss: 0.045907\n",
            "train:  67% 20/30 [00:34<00:16,  1.68s/it]Train Epoch: 47 [20/30 (67%)] Loss: 0.050321\n",
            "train:  97% 29/30 [00:51<00:01,  1.79s/it]\n",
            "    epoch          : 47\n",
            "    loss           : 0.046493833015362425\n",
            "    grad norm      : 0.19891687085231144\n",
            "    WER (argmax)   : 0.04968547007708802\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    CER (argmax)   : 0.022299149864922044\n",
            "Train Epoch: 48 [0/30 (0%)] Loss: 0.042618\n",
            "train:  33% 10/30 [00:17<00:32,  1.65s/it]Train Epoch: 48 [10/30 (33%)] Loss: 0.041482\n",
            "train:  67% 20/30 [00:34<00:16,  1.66s/it]Train Epoch: 48 [20/30 (67%)] Loss: 0.039149\n",
            "train:  97% 29/30 [00:51<00:01,  1.77s/it]\n",
            "    epoch          : 48\n",
            "    loss           : 0.04096182038386663\n",
            "    grad norm      : 0.12732121000687283\n",
            "train:   0% 0/30 [00:00<?, ?it/s]    WER (argmax)   : 0.04554798071492651\n",
            "    CER (argmax)   : 0.02122521519200298\n",
            "Train Epoch: 49 [0/30 (0%)] Loss: 0.038837\n",
            "train:  33% 10/30 [00:17<00:33,  1.68s/it]Train Epoch: 49 [10/30 (33%)] Loss: 0.047300\n",
            "train:  67% 20/30 [00:34<00:16,  1.66s/it]Train Epoch: 49 [20/30 (67%)] Loss: 0.041089\n",
            "train:  97% 29/30 [00:50<00:01,  1.66s/it]    epoch          : 49\n",
            "train:  97% 29/30 [00:51<00:01,  1.79s/it]\n",
            "    loss           : 0.04285045340657234\n",
            "    grad norm      : 0.16232123225927353\n",
            "    WER (argmax)   : 0.045833841268716334\n",
            "    CER (argmax)   : 0.022947179002838367\n",
            "train:   0% 0/30 [00:00<?, ?it/s]Train Epoch: 50 [0/30 (0%)] Loss: 0.038614\n",
            "train:  33% 10/30 [00:17<00:32,  1.64s/it]Train Epoch: 50 [10/30 (33%)] Loss: 0.037133\n",
            "train:  67% 20/30 [00:34<00:17,  1.78s/it]Train Epoch: 50 [20/30 (67%)] Loss: 0.046054\n",
            "train:  97% 29/30 [00:52<00:01,  1.80s/it]\n",
            "    epoch          : 50\n",
            "    loss           : 0.04058948668340842\n",
            "    grad norm      : 0.14744942610462505\n",
            "    WER (argmax)   : 0.044092114011862624\n",
            "    CER (argmax)   : 0.022116622476431525\n",
            "Saving checkpoint: saved/models/overfit_config/1016_16_49_17:654/checkpoint-epoch50.pth ...\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 18240... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    CER (argmax)_train ██████████████▇▄▂▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    WER (argmax)_train ███████████████▇▄▂▇▃▂▂▁▁▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                epoch_ ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch_train ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       grad norm_train ▆█▆▃▂▁▁▁▁▁▁▂▂▃▃▂▂▁▂▁▁▂▁▁▁▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   learning rate_train ▁▁▁▁▁▂▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇▇██████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_train █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   steps_per_sec_train ██▇██▅██▇▇▇▇▆▅▅▆▅▅▆▂▄▄▅▄▄▄▄▃▂▄▄▄▂▄▄▃▄▁▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    CER (argmax)_train 0.02223\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    WER (argmax)_train 0.0443\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                epoch_ 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch_train 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       grad norm_train 0.16098\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   learning rate_train 0.00385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_train 0.04087\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   steps_per_sec_train 0.057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 450 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcool-sea-60\u001b[0m: \u001b[34mhttps://wandb.ai/ivan-gorin/asr_project/runs/24xhrfs7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211016_164920-24xhrfs7/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
          ]
        }
      ]
    }
  ]
}